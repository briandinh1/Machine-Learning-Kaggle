{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 94323 is out of bounds for axis 0 with size 80000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-336700fabee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnUse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m             \u001b[1;31m# this is a lot faster than the bagging loop:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[1;31m# Better: set dY = gradient of loss at soft predictions Pt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtreeRegress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxDepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# train and save learner\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[1;31m#Pt += step*tree.predict(X)[:,0]       # predict on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mPt2\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m       \u001b[1;31m# predict on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brian\\Anaconda2\\mltools\\dtree.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0mSee\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m       \"\"\"\n\u001b[0;32m--> 251\u001b[0;31m       \u001b[0mtreeBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtreeBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brian\\Anaconda2\\mltools\\dtree.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m     \u001b[1;31m# if we were given optional arguments,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#  just pass them through to \"train\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brian\\Anaconda2\\mltools\\dtree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, minParent, maxDepth, minLeaf, nFeatures)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m              \u001b[1;31m# start building at the root\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__train_recursive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminParent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxDepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminLeaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnFeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m]\u001b[0m                              \u001b[1;31m# store returned data into object\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brian\\Anaconda2\\mltools\\dtree.pyc\u001b[0m in \u001b[0;36m__train_recursive\u001b[0;34m(self, X, Y, depth, minParent, maxDepth, minLeaf, nFeatures)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mdsorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi_feat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# sort data...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi_feat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m                               \u001b[1;31m# ...get sorted indices...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtsorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m]\u001b[0m                                              \u001b[1;31m# ...and sort targets by feature ID\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mcan_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsorted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdsorted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# which indices are valid split points?\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[1;31m# TODO: numeric comparison instead?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 94323 is out of bounds for axis 0 with size 80000"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import mltools as ml\n",
    "import matplotlib.pyplot as plt   # use matplotlib for plotting with inline plots\n",
    "\n",
    "X = np.genfromtxt(\"data/X_train.txt\",delimiter=' ')\n",
    "Y = np.genfromtxt(\"data/Y_train.txt\",delimiter=' ')\n",
    "Xt,Xv,Yt,Yv = ml.splitData(X,Y,0.80)\n",
    "\n",
    "Xe = np.genfromtxt('data/X_test.txt',delimiter=' ')\n",
    "\n",
    "\n",
    "def auc(soft,Y):\n",
    "    \"\"\"Manual AUC function for applying to soft prediction vectors\"\"\"\n",
    "    indices = np.argsort(soft)         # sort data by score value\n",
    "    Y = Y[indices]\n",
    "    sorted_soft = soft[indices]\n",
    "    \n",
    "    # compute rank (averaged for ties) of sorted data\n",
    "    dif = np.hstack( ([True],np.diff(sorted_soft)!=0,[True]) )\n",
    "    r1  = np.argwhere(dif).flatten()\n",
    "    r2  = r1[0:-1] + 0.5*(r1[1:]-r1[0:-1]) + 0.5\n",
    "    rnk = r2[np.cumsum(dif[:-1])-1]\n",
    "    \n",
    "    # number of true negatives and positives\n",
    "    n0,n1 = sum(Y == 0), sum(Y == 1)\n",
    "    \n",
    "    # compute AUC using Mann-Whitney U statistic\n",
    "    result = (np.sum(rnk[Y == 1]) - n1 * (n1 + 1.0) / 2.0) / n1 / n0\n",
    "    return result\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "nUse= 300\n",
    "# mu = Y.mean()  #mu = Yt.mean()\n",
    "# dy = Y - mu   #dY = Yt - mu\n",
    "mu = Yt.mean()\n",
    "dY = Yt - mu\n",
    "step = 0.5\n",
    "P = np.zeros((X.shape[0],))+mu\n",
    "Pt2 = np.zeros((Xt.shape[0],))+mu\n",
    "Pv2 = np.zeros((Xv.shape[0],))+mu\n",
    "Pe2 = np.zeros((Xe.shape[0],))+mu\n",
    "\n",
    "np.random.seed(0)\n",
    "for l in range(nUse):             # this is a lot faster than the bagging loop:\n",
    "    # Better: set dY = gradient of loss at soft predictions Pt\n",
    "    tree = ml.dtree.treeRegress(X,dY[:,np.newaxis], maxDepth=5)  # train and save learner\n",
    "    #P += step*tree.predict(X)[:,0]       # predict on training data\n",
    "    Pt2 += step*tree.predict(Xt)[:,0]       # predict on training data\n",
    "    Pv2 += step*tree.predict(Xv)[:,0]        #    and validation data\n",
    "    Pe2 += step*tree.predict(Xe)[:,0]        #    and test data\n",
    "    #dY  -= step*tree.predict(X)[:,0]        # update residual for next learner\n",
    "    dY  -= step*tree.predict(Xt)[:,0]        # update residual for next learner\n",
    "\n",
    "    #print \" {} trees: MSE ~ {};  AUC - {};\".format(l+1, ((Y-P)**2).mean(), auc(P,Y) )\n",
    "    print \" {} trees: MSE ~ {};  AUC - {};\".format(l+1, ((Yv-Pv2)**2).mean(), auc(Pv2,Yv) )\n",
    "\n",
    "print \"AUC: \", auc(Pv2,Yv)\n",
    "#print \"AUC: \", auc(P,Y)\n",
    "\n",
    "#np.savetxt('kaggle prediction.txt',np.vstack( (np.arange(len(Pe2)) , Pe2) ).T,'%d, %.2f',header='ID,Prob1',comments='',delimiter=',');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
