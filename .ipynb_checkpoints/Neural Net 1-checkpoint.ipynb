{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.loadtxt('data/X_train.txt')\n",
    "Y = np.loadtxt('data/Y_train.txt')\n",
    "Xe = np.loadtxt('data/X_test.txt')\n",
    "\n",
    "Xt,Xv,Yt,Yv = ml.splitData(X,Y,0.80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(Xt)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "Xt_new = scaler.transform(Xt)\n",
    "Xv_new = scaler.transform(Xv)\n",
    "X_new = scaler.transform(X)\n",
    "Xe_new = scaler.transform(Xe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60607766\n",
      "Iteration 2, loss = 0.60085192\n",
      "Iteration 3, loss = 0.59779169\n",
      "Iteration 4, loss = 0.59614290\n",
      "Iteration 5, loss = 0.59461126\n",
      "Iteration 6, loss = 0.59375017\n",
      "Iteration 7, loss = 0.59256045\n",
      "Iteration 8, loss = 0.59179727\n",
      "Iteration 9, loss = 0.59127288\n",
      "Iteration 10, loss = 0.59038334\n",
      "Iteration 11, loss = 0.58976730\n",
      "Iteration 12, loss = 0.58919242\n",
      "Iteration 13, loss = 0.58885648\n",
      "Iteration 14, loss = 0.58819533\n",
      "Iteration 15, loss = 0.58760818\n",
      "Iteration 16, loss = 0.58747259\n",
      "Iteration 17, loss = 0.58665124\n",
      "Iteration 18, loss = 0.58650879\n",
      "Iteration 19, loss = 0.58589021\n",
      "Iteration 20, loss = 0.58585994\n",
      "Iteration 21, loss = 0.58549883\n",
      "Iteration 22, loss = 0.58503809\n",
      "Iteration 23, loss = 0.58470972\n",
      "Iteration 24, loss = 0.58423081\n",
      "Iteration 25, loss = 0.58380577\n",
      "Iteration 26, loss = 0.58343278\n",
      "Iteration 27, loss = 0.58295953\n",
      "Iteration 28, loss = 0.58275991\n",
      "Iteration 29, loss = 0.58262768\n",
      "Iteration 30, loss = 0.58211502\n",
      "Iteration 31, loss = 0.58159785\n",
      "Iteration 32, loss = 0.58142544\n",
      "Iteration 33, loss = 0.58077963\n",
      "Iteration 34, loss = 0.58058044\n",
      "Iteration 35, loss = 0.58035413\n",
      "Iteration 36, loss = 0.57977102\n",
      "Iteration 37, loss = 0.57979812\n",
      "Iteration 38, loss = 0.57928579\n",
      "Iteration 39, loss = 0.57899820\n",
      "Iteration 40, loss = 0.57900558\n",
      "Iteration 41, loss = 0.57869865\n",
      "Iteration 42, loss = 0.57815912\n",
      "Iteration 43, loss = 0.57785048\n",
      "Iteration 44, loss = 0.57736016\n",
      "Iteration 45, loss = 0.57731069\n",
      "Iteration 46, loss = 0.57689069\n",
      "Iteration 47, loss = 0.57639827\n",
      "Iteration 48, loss = 0.57642640\n",
      "Iteration 49, loss = 0.57608410\n",
      "Iteration 50, loss = 0.57560340\n",
      "Iteration 51, loss = 0.57553319\n",
      "Iteration 52, loss = 0.57506104\n",
      "Iteration 53, loss = 0.57496987\n",
      "Iteration 54, loss = 0.57471587\n",
      "Iteration 55, loss = 0.57412362\n",
      "Iteration 56, loss = 0.57402849\n",
      "Iteration 57, loss = 0.57350157\n",
      "Iteration 58, loss = 0.57355265\n",
      "Iteration 59, loss = 0.57317472\n",
      "Iteration 60, loss = 0.57297691\n",
      "Iteration 61, loss = 0.57276212\n",
      "Iteration 62, loss = 0.57248587\n",
      "Iteration 63, loss = 0.57222085\n",
      "Iteration 64, loss = 0.57174313\n",
      "Iteration 65, loss = 0.57171388\n",
      "Iteration 66, loss = 0.57140241\n",
      "Iteration 67, loss = 0.57075861\n",
      "Iteration 68, loss = 0.57066784\n",
      "Iteration 69, loss = 0.57059001\n",
      "Iteration 70, loss = 0.57053959\n",
      "Iteration 71, loss = 0.57004382\n",
      "Iteration 72, loss = 0.56982016\n",
      "Iteration 73, loss = 0.56921496\n",
      "Iteration 74, loss = 0.56925730\n",
      "Iteration 75, loss = 0.56896875\n",
      "Iteration 76, loss = 0.56880780\n",
      "Iteration 77, loss = 0.56851226\n",
      "Iteration 78, loss = 0.56813855\n",
      "Iteration 79, loss = 0.56786870\n",
      "Iteration 80, loss = 0.56764016\n",
      "Iteration 81, loss = 0.56744062\n",
      "Iteration 82, loss = 0.56697724\n",
      "Iteration 83, loss = 0.56666669\n",
      "Iteration 84, loss = 0.56661715\n",
      "Iteration 85, loss = 0.56615655\n",
      "Iteration 86, loss = 0.56579335\n",
      "Iteration 87, loss = 0.56586069\n",
      "Iteration 88, loss = 0.56510845\n",
      "Iteration 89, loss = 0.56497633\n",
      "Iteration 90, loss = 0.56513569\n",
      "Iteration 91, loss = 0.56446928\n",
      "Iteration 92, loss = 0.56448679\n",
      "Iteration 93, loss = 0.56408937\n",
      "Iteration 94, loss = 0.56357289\n",
      "Iteration 95, loss = 0.56344015\n",
      "Iteration 96, loss = 0.56292619\n",
      "Iteration 97, loss = 0.56281377\n",
      "Iteration 98, loss = 0.56261140\n",
      "Iteration 99, loss = 0.56231048\n",
      "Iteration 100, loss = 0.56184324\n",
      "Iteration 101, loss = 0.56168251\n",
      "Iteration 102, loss = 0.56151131\n",
      "Iteration 103, loss = 0.56129419\n",
      "Iteration 104, loss = 0.56083463\n",
      "Iteration 105, loss = 0.56059892\n",
      "Iteration 106, loss = 0.56019485\n",
      "Iteration 107, loss = 0.56005171\n",
      "Iteration 108, loss = 0.55955026\n",
      "Iteration 109, loss = 0.55923340\n",
      "Iteration 110, loss = 0.55906585\n",
      "Iteration 111, loss = 0.55892681\n",
      "Iteration 112, loss = 0.55841530\n",
      "Iteration 113, loss = 0.55820711\n",
      "Iteration 114, loss = 0.55789347\n",
      "Iteration 115, loss = 0.55756889\n",
      "Iteration 116, loss = 0.55735685\n",
      "Iteration 117, loss = 0.55692681\n",
      "Iteration 118, loss = 0.55688306\n",
      "Iteration 119, loss = 0.55633389\n",
      "Iteration 120, loss = 0.55619932\n",
      "Iteration 121, loss = 0.55600735\n",
      "Iteration 122, loss = 0.55547277\n",
      "Iteration 123, loss = 0.55525769\n",
      "Iteration 124, loss = 0.55505710\n",
      "Iteration 125, loss = 0.55487010\n",
      "Iteration 126, loss = 0.55460674\n",
      "Iteration 127, loss = 0.55413017\n",
      "Iteration 128, loss = 0.55388407\n",
      "Iteration 129, loss = 0.55341913\n",
      "Iteration 130, loss = 0.55338135\n",
      "Iteration 131, loss = 0.55297247\n",
      "Iteration 132, loss = 0.55244024\n",
      "Iteration 133, loss = 0.55243968\n",
      "Iteration 134, loss = 0.55206313\n",
      "Iteration 135, loss = 0.55185339\n",
      "Iteration 136, loss = 0.55157761\n",
      "Iteration 137, loss = 0.55121727\n",
      "Iteration 138, loss = 0.55078312\n",
      "Iteration 139, loss = 0.55079968\n",
      "Iteration 140, loss = 0.55049547\n",
      "Iteration 141, loss = 0.55004235\n",
      "Iteration 142, loss = 0.54973613\n",
      "Iteration 143, loss = 0.54935807\n",
      "Iteration 144, loss = 0.54915833\n",
      "Iteration 145, loss = 0.54890311\n",
      "Iteration 146, loss = 0.54840853\n",
      "Iteration 147, loss = 0.54816529\n",
      "Iteration 148, loss = 0.54787218\n",
      "Iteration 149, loss = 0.54754716\n",
      "Iteration 150, loss = 0.54737047\n",
      "Iteration 151, loss = 0.54703724\n",
      "Iteration 152, loss = 0.54661997\n",
      "Iteration 153, loss = 0.54648284\n",
      "Iteration 154, loss = 0.54615395\n",
      "Iteration 155, loss = 0.54575629\n",
      "Iteration 156, loss = 0.54561618\n",
      "Iteration 157, loss = 0.54537905\n",
      "Iteration 158, loss = 0.54511979\n",
      "Iteration 159, loss = 0.54466848\n",
      "Iteration 160, loss = 0.54423653\n",
      "Iteration 161, loss = 0.54410980\n",
      "Iteration 162, loss = 0.54365470\n",
      "Iteration 163, loss = 0.54352703\n",
      "Iteration 164, loss = 0.54317547\n",
      "Iteration 165, loss = 0.54294342\n",
      "Iteration 166, loss = 0.54252313\n",
      "Iteration 167, loss = 0.54224867\n",
      "Iteration 168, loss = 0.54186185\n",
      "Iteration 169, loss = 0.54162290\n",
      "Iteration 170, loss = 0.54116112\n",
      "Iteration 171, loss = 0.54079488\n",
      "Iteration 172, loss = 0.54068398\n",
      "Iteration 173, loss = 0.54038574\n",
      "Iteration 174, loss = 0.54009494\n",
      "Iteration 175, loss = 0.53942976\n",
      "Iteration 176, loss = 0.53937122\n",
      "Iteration 177, loss = 0.53894733\n",
      "Iteration 178, loss = 0.53893785\n",
      "Iteration 179, loss = 0.53834966\n",
      "Iteration 180, loss = 0.53814831\n",
      "Iteration 181, loss = 0.53779188\n",
      "Iteration 182, loss = 0.53724137\n",
      "Iteration 183, loss = 0.53728809\n",
      "Iteration 184, loss = 0.53669604\n",
      "Iteration 185, loss = 0.53644110\n",
      "Iteration 186, loss = 0.53614699\n",
      "Iteration 187, loss = 0.53592588\n",
      "Iteration 188, loss = 0.53555786\n",
      "Iteration 189, loss = 0.53512305\n",
      "Iteration 190, loss = 0.53491014\n",
      "Iteration 191, loss = 0.53464628\n",
      "Iteration 192, loss = 0.53426034\n",
      "Iteration 193, loss = 0.53386484\n",
      "Iteration 194, loss = 0.53354684\n",
      "Iteration 195, loss = 0.53328399\n",
      "Iteration 196, loss = 0.53286365\n",
      "Iteration 197, loss = 0.53292500\n",
      "Iteration 198, loss = 0.53202722\n",
      "Iteration 199, loss = 0.53210978\n",
      "Iteration 200, loss = 0.53159546\n",
      "Iteration 201, loss = 0.53112954\n",
      "Iteration 202, loss = 0.53101876\n",
      "Iteration 203, loss = 0.53050742\n",
      "Iteration 204, loss = 0.53037387\n",
      "Iteration 205, loss = 0.53020166\n",
      "Iteration 206, loss = 0.52981771\n",
      "Iteration 207, loss = 0.52948478\n",
      "Iteration 208, loss = 0.52918800\n",
      "Iteration 209, loss = 0.52878181\n",
      "Iteration 210, loss = 0.52835652\n",
      "Iteration 211, loss = 0.52796506\n",
      "Iteration 212, loss = 0.52774963\n",
      "Iteration 213, loss = 0.52741732\n",
      "Iteration 214, loss = 0.52697636\n",
      "Iteration 215, loss = 0.52664996\n",
      "Iteration 216, loss = 0.52621504\n",
      "Iteration 217, loss = 0.52591764\n",
      "Iteration 218, loss = 0.52564161\n",
      "Iteration 219, loss = 0.52533053\n",
      "Iteration 220, loss = 0.52499142\n",
      "Iteration 221, loss = 0.52471938\n",
      "Iteration 222, loss = 0.52422349\n",
      "Iteration 223, loss = 0.52376760\n",
      "Iteration 224, loss = 0.52371616\n",
      "Iteration 225, loss = 0.52316214\n",
      "Iteration 226, loss = 0.52290919\n",
      "Iteration 227, loss = 0.52263116\n",
      "Iteration 228, loss = 0.52229124\n",
      "Iteration 229, loss = 0.52203713\n",
      "Iteration 230, loss = 0.52145817\n",
      "Iteration 231, loss = 0.52136612\n",
      "Iteration 232, loss = 0.52091140\n",
      "Iteration 233, loss = 0.52058910\n",
      "Iteration 234, loss = 0.52030085\n",
      "Iteration 235, loss = 0.51979826\n",
      "Iteration 236, loss = 0.51953006\n",
      "Iteration 237, loss = 0.51926388\n",
      "Iteration 238, loss = 0.51857087\n",
      "Iteration 239, loss = 0.51849507\n",
      "Iteration 240, loss = 0.51791703\n",
      "Iteration 241, loss = 0.51758273\n",
      "Iteration 242, loss = 0.51724225\n",
      "Iteration 243, loss = 0.51715223\n",
      "Iteration 244, loss = 0.51665870\n",
      "Iteration 245, loss = 0.51640876\n",
      "Iteration 246, loss = 0.51612919\n",
      "Iteration 247, loss = 0.51559934\n",
      "Iteration 248, loss = 0.51512297\n",
      "Iteration 249, loss = 0.51483196\n",
      "Iteration 250, loss = 0.51442968\n",
      "Iteration 251, loss = 0.51416316\n",
      "Iteration 252, loss = 0.51406236\n",
      "Iteration 253, loss = 0.51375370\n",
      "Iteration 254, loss = 0.51310192\n",
      "Iteration 255, loss = 0.51279577\n",
      "Iteration 256, loss = 0.51244879\n",
      "Iteration 257, loss = 0.51196002\n",
      "Iteration 258, loss = 0.51172587\n",
      "Iteration 259, loss = 0.51110940\n",
      "Iteration 260, loss = 0.51115102\n",
      "Iteration 261, loss = 0.51083649\n",
      "Iteration 262, loss = 0.51024659\n",
      "Iteration 263, loss = 0.51004351\n",
      "Iteration 264, loss = 0.50986761\n",
      "Iteration 265, loss = 0.50916385\n",
      "Iteration 266, loss = 0.50876003\n",
      "Iteration 267, loss = 0.50888875\n",
      "Iteration 268, loss = 0.50814934\n",
      "Iteration 269, loss = 0.50785858\n",
      "Iteration 270, loss = 0.50717384\n",
      "Iteration 271, loss = 0.50733767\n",
      "Iteration 272, loss = 0.50652319\n",
      "Iteration 273, loss = 0.50642405\n",
      "Iteration 274, loss = 0.50613743\n",
      "Iteration 275, loss = 0.50549833\n",
      "Iteration 276, loss = 0.50530675\n",
      "Iteration 277, loss = 0.50507310\n",
      "Iteration 278, loss = 0.50442341\n",
      "Iteration 279, loss = 0.50436189\n",
      "Iteration 280, loss = 0.50381231\n",
      "Iteration 281, loss = 0.50357600\n",
      "Iteration 282, loss = 0.50304342\n",
      "Iteration 283, loss = 0.50296762\n",
      "Iteration 284, loss = 0.50248474\n",
      "Iteration 285, loss = 0.50215232\n",
      "Iteration 286, loss = 0.50180588\n",
      "Iteration 287, loss = 0.50112023\n",
      "Iteration 288, loss = 0.50096868\n",
      "Iteration 289, loss = 0.50068307\n",
      "Iteration 290, loss = 0.50007039\n",
      "Iteration 291, loss = 0.50000411\n",
      "Iteration 292, loss = 0.49961694\n",
      "Iteration 293, loss = 0.49935237\n",
      "Iteration 294, loss = 0.49905268\n",
      "Iteration 295, loss = 0.49850162\n",
      "Iteration 296, loss = 0.49808578\n",
      "Iteration 297, loss = 0.49784617\n",
      "Iteration 298, loss = 0.49749663\n",
      "Iteration 299, loss = 0.49706888\n",
      "Iteration 300, loss = 0.49668109\n",
      "Iteration 301, loss = 0.49642390\n",
      "Iteration 302, loss = 0.49613026\n",
      "Iteration 303, loss = 0.49563296\n",
      "Iteration 304, loss = 0.49519632\n",
      "Iteration 305, loss = 0.49491165\n",
      "Iteration 306, loss = 0.49459136\n",
      "Iteration 307, loss = 0.49429612\n",
      "Iteration 308, loss = 0.49371863\n",
      "Iteration 309, loss = 0.49342791\n",
      "Iteration 310, loss = 0.49313091\n",
      "Iteration 311, loss = 0.49303335\n",
      "Iteration 312, loss = 0.49245948\n",
      "Iteration 313, loss = 0.49223399\n",
      "Iteration 314, loss = 0.49131891\n",
      "Iteration 315, loss = 0.49124658\n",
      "Iteration 316, loss = 0.49095909\n",
      "Iteration 317, loss = 0.49056492\n",
      "Iteration 318, loss = 0.49004459\n",
      "Iteration 319, loss = 0.48995214\n",
      "Iteration 320, loss = 0.48971957\n",
      "Iteration 321, loss = 0.48929854\n",
      "Iteration 322, loss = 0.48877181\n",
      "Iteration 323, loss = 0.48865720\n",
      "Iteration 324, loss = 0.48810592\n",
      "Iteration 325, loss = 0.48771892\n",
      "Iteration 326, loss = 0.48741312\n",
      "Iteration 327, loss = 0.48692968\n",
      "Iteration 328, loss = 0.48678997\n",
      "Iteration 329, loss = 0.48640000\n",
      "Iteration 330, loss = 0.48583386\n",
      "Iteration 331, loss = 0.48569382\n",
      "Iteration 332, loss = 0.48515232\n",
      "Iteration 333, loss = 0.48476071\n",
      "Iteration 334, loss = 0.48432049\n",
      "Iteration 335, loss = 0.48415550\n",
      "Iteration 336, loss = 0.48373876\n",
      "Iteration 337, loss = 0.48330212\n",
      "Iteration 338, loss = 0.48304631\n",
      "Iteration 339, loss = 0.48268273\n",
      "Iteration 340, loss = 0.48252738\n",
      "Iteration 341, loss = 0.48182116\n",
      "Iteration 342, loss = 0.48181461\n",
      "Iteration 343, loss = 0.48111129\n",
      "Iteration 344, loss = 0.48121600\n",
      "Iteration 345, loss = 0.48065697\n",
      "Iteration 346, loss = 0.48014392\n",
      "Iteration 347, loss = 0.47953534\n",
      "Iteration 348, loss = 0.47956931\n",
      "Iteration 349, loss = 0.47909582\n",
      "Iteration 350, loss = 0.47869261\n",
      "Iteration 351, loss = 0.47840193\n",
      "Iteration 352, loss = 0.47831249\n",
      "Iteration 353, loss = 0.47755672\n",
      "Iteration 354, loss = 0.47725291\n",
      "Iteration 355, loss = 0.47678999\n",
      "Iteration 356, loss = 0.47692059\n",
      "Iteration 357, loss = 0.47618569\n",
      "Iteration 358, loss = 0.47573137\n",
      "Iteration 359, loss = 0.47556198\n",
      "Iteration 360, loss = 0.47517209\n",
      "Iteration 361, loss = 0.47499790\n",
      "Iteration 362, loss = 0.47442620\n",
      "Iteration 363, loss = 0.47401739\n",
      "Iteration 364, loss = 0.47373782\n",
      "Iteration 365, loss = 0.47334853\n",
      "Iteration 366, loss = 0.47317909\n",
      "Iteration 367, loss = 0.47268906\n",
      "Iteration 368, loss = 0.47240791\n",
      "Iteration 369, loss = 0.47214065\n",
      "Iteration 370, loss = 0.47148785\n",
      "Iteration 371, loss = 0.47106129\n",
      "Iteration 372, loss = 0.47097414\n",
      "Iteration 373, loss = 0.47052439\n",
      "Iteration 374, loss = 0.47027801\n",
      "Iteration 375, loss = 0.47000757\n",
      "Iteration 376, loss = 0.46953270\n",
      "Iteration 377, loss = 0.46928075\n",
      "Iteration 378, loss = 0.46895535\n",
      "Iteration 379, loss = 0.46859344\n",
      "Iteration 380, loss = 0.46786238\n",
      "Iteration 381, loss = 0.46756143\n",
      "Iteration 382, loss = 0.46730424\n",
      "Iteration 383, loss = 0.46710429\n",
      "Iteration 384, loss = 0.46663721\n",
      "Iteration 385, loss = 0.46614811\n",
      "Iteration 386, loss = 0.46609483\n",
      "Iteration 387, loss = 0.46560930\n",
      "Iteration 388, loss = 0.46515732\n",
      "Iteration 389, loss = 0.46473390\n",
      "Iteration 390, loss = 0.46454826\n",
      "Iteration 391, loss = 0.46445866\n",
      "Iteration 392, loss = 0.46386524\n",
      "Iteration 393, loss = 0.46340970\n",
      "Iteration 394, loss = 0.46306934\n",
      "Iteration 395, loss = 0.46265671\n",
      "Iteration 396, loss = 0.46261558\n",
      "Iteration 397, loss = 0.46215041\n",
      "Iteration 398, loss = 0.46176621\n",
      "Iteration 399, loss = 0.46133617\n",
      "Iteration 400, loss = 0.46101362\n",
      "Iteration 401, loss = 0.46056470\n",
      "Iteration 402, loss = 0.46057031\n",
      "Iteration 403, loss = 0.46009662\n",
      "Iteration 404, loss = 0.45961229\n",
      "Iteration 405, loss = 0.45921886\n",
      "Iteration 406, loss = 0.45865904\n",
      "Iteration 407, loss = 0.45852725\n",
      "Iteration 408, loss = 0.45809548\n",
      "Iteration 409, loss = 0.45808334\n",
      "Iteration 410, loss = 0.45753189\n",
      "Iteration 411, loss = 0.45748212\n",
      "Iteration 412, loss = 0.45672422\n",
      "Iteration 413, loss = 0.45634582\n",
      "Iteration 414, loss = 0.45599697\n",
      "Iteration 415, loss = 0.45580215\n",
      "Iteration 416, loss = 0.45538629\n",
      "Iteration 417, loss = 0.45512019\n",
      "Iteration 418, loss = 0.45457856\n",
      "Iteration 419, loss = 0.45435968\n",
      "Iteration 420, loss = 0.45400520\n",
      "Iteration 421, loss = 0.45360619\n",
      "Iteration 422, loss = 0.45333651\n",
      "Iteration 423, loss = 0.45301098\n",
      "Iteration 424, loss = 0.45282555\n",
      "Iteration 425, loss = 0.45220356\n",
      "Iteration 426, loss = 0.45195020\n",
      "Iteration 427, loss = 0.45159622\n",
      "Iteration 428, loss = 0.45123293\n",
      "Iteration 429, loss = 0.45103014\n",
      "Iteration 430, loss = 0.45064287\n",
      "Iteration 431, loss = 0.45002218\n",
      "Iteration 432, loss = 0.44996919\n",
      "Iteration 433, loss = 0.44963361\n",
      "Iteration 434, loss = 0.44923391\n",
      "Iteration 435, loss = 0.44919608\n",
      "Iteration 436, loss = 0.44844818\n",
      "Iteration 437, loss = 0.44819127\n",
      "Iteration 438, loss = 0.44791924\n",
      "Iteration 439, loss = 0.44745471\n",
      "Iteration 440, loss = 0.44706524\n",
      "Iteration 441, loss = 0.44699089\n",
      "Iteration 442, loss = 0.44639641\n",
      "Iteration 443, loss = 0.44623928\n",
      "Iteration 444, loss = 0.44553626\n",
      "Iteration 445, loss = 0.44529311\n",
      "Iteration 446, loss = 0.44516035\n",
      "Iteration 447, loss = 0.44455803\n",
      "Iteration 448, loss = 0.44431049\n",
      "Iteration 449, loss = 0.44392063\n",
      "Iteration 450, loss = 0.44368995\n",
      "Iteration 451, loss = 0.44339314\n",
      "Iteration 452, loss = 0.44306959\n",
      "Iteration 453, loss = 0.44260404\n",
      "Iteration 454, loss = 0.44240051\n",
      "Iteration 455, loss = 0.44202171\n",
      "Iteration 456, loss = 0.44164133\n",
      "Iteration 457, loss = 0.44138702\n",
      "Iteration 458, loss = 0.44119487\n",
      "Iteration 459, loss = 0.44059661\n",
      "Iteration 460, loss = 0.44037441\n",
      "Iteration 461, loss = 0.43990728\n",
      "Iteration 462, loss = 0.43965899\n",
      "Iteration 463, loss = 0.43927485\n",
      "Iteration 464, loss = 0.43880008\n",
      "Iteration 465, loss = 0.43869206\n",
      "Iteration 466, loss = 0.43813730\n",
      "Iteration 467, loss = 0.43798598\n",
      "Iteration 468, loss = 0.43765991\n",
      "Iteration 469, loss = 0.43720886\n",
      "Iteration 470, loss = 0.43680043\n",
      "Iteration 471, loss = 0.43666070\n",
      "Iteration 472, loss = 0.43630576\n",
      "Iteration 473, loss = 0.43590158\n",
      "Iteration 474, loss = 0.43559797\n",
      "Iteration 475, loss = 0.43529418\n",
      "Iteration 476, loss = 0.43482394\n",
      "Iteration 477, loss = 0.43462434\n",
      "Iteration 478, loss = 0.43426095\n",
      "Iteration 479, loss = 0.43383711\n",
      "Iteration 480, loss = 0.43364263\n",
      "Iteration 481, loss = 0.43334238\n",
      "Iteration 482, loss = 0.43294893\n",
      "Iteration 483, loss = 0.43289073\n",
      "Iteration 484, loss = 0.43227632\n",
      "Iteration 485, loss = 0.43167548\n",
      "Iteration 486, loss = 0.43166446\n",
      "Iteration 487, loss = 0.43107943\n",
      "Iteration 488, loss = 0.43074235\n",
      "Iteration 489, loss = 0.43051350\n",
      "Iteration 490, loss = 0.43017384\n",
      "Iteration 491, loss = 0.42974691\n",
      "Iteration 492, loss = 0.42937869\n",
      "Iteration 493, loss = 0.42936304\n",
      "Iteration 494, loss = 0.42894530\n",
      "Iteration 495, loss = 0.42849524\n",
      "Iteration 496, loss = 0.42824968\n",
      "Iteration 497, loss = 0.42808971\n",
      "Iteration 498, loss = 0.42740764\n",
      "Iteration 499, loss = 0.42720716\n",
      "Iteration 500, loss = 0.42701761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brian\\Anaconda2\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(200, 200, 200), learning_rate='constant',\n",
       "       learning_rate_init=0.0001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=1e-06, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# You should monitor its performance, preferably on both training & validation data, during backpropagation, \n",
    "# and verify that the training process is working properly and converging to a reasonable performance value \n",
    "# (e.g., comparably to other methods).  Start with few layers (2-3) and moderate numbers of hidden nodes (100-1000) per layer; \n",
    "# within these settings you can work to make sure your model is training adequately.\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(200,200,200), activation='tanh', solver='adam', max_iter=500, shuffle=True, \n",
    "                    learning_rate_init=0.0001, tol=1e-6, verbose=True, early_stopping=False)\n",
    "clf.fit(X_new,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print 'Training Score =',clf.score(Xt,Yt)\n",
    "# Ytpred = clf.predict_proba(Xt)\n",
    "# print 'Training AUC =',roc_auc_score(Yt, Ytpred[:,1])\n",
    "# print\n",
    "# print 'Validation Score =',clf.score(Xv,Yv)\n",
    "# Yvpred = clf.predict_proba(Xv)\n",
    "# print 'Validation AUC =',roc_auc_score(Yv, Yvpred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "YtePred = clf.predict_proba(Xe_new)[:,1]\n",
    "np.savetxt('Yhat_NNet.txt',\n",
    "    np.vstack( (np.arange(len(YtePred)) , YtePred) ).T,\n",
    "    '%d, %.2f',header='ID,Prob1',comments='',delimiter=',');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.48949339  0.14610884  0.63234873 ...,  0.07059832  0.43257053\n",
      "  0.32292375]\n"
     ]
    }
   ],
   "source": [
    "print YtePred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 trees: MSE ~ 0.210517953699;  AUC - 0.661809457998;\n",
      " 2 trees: MSE ~ 0.205166608722;  AUC - 0.67307323957;\n",
      " 3 trees: MSE ~ 0.202089968131;  AUC - 0.682243508848;\n",
      " 4 trees: MSE ~ 0.200593056759;  AUC - 0.686881825936;\n",
      " 5 trees: MSE ~ 0.199531460312;  AUC - 0.690350106887;\n",
      " 6 trees: MSE ~ 0.197964396854;  AUC - 0.697126559022;\n",
      " 7 trees: MSE ~ 0.196768798894;  AUC - 0.702018522519;\n",
      " 8 trees: MSE ~ 0.196130040292;  AUC - 0.704726859623;\n",
      " 9 trees: MSE ~ 0.195663177827;  AUC - 0.707434098142;\n",
      " 10 trees: MSE ~ 0.194895655061;  AUC - 0.70949843929;\n",
      " 11 trees: MSE ~ 0.194706610378;  AUC - 0.710023290357;\n",
      " 12 trees: MSE ~ 0.19420378569;  AUC - 0.711871813698;\n",
      " 13 trees: MSE ~ 0.19329583296;  AUC - 0.715128509702;\n",
      " 14 trees: MSE ~ 0.192488781719;  AUC - 0.717965475482;\n",
      " 15 trees: MSE ~ 0.19177472882;  AUC - 0.72085153154;\n",
      " 16 trees: MSE ~ 0.191613745754;  AUC - 0.721512388425;\n",
      " 17 trees: MSE ~ 0.19099900717;  AUC - 0.724007769074;\n",
      " 18 trees: MSE ~ 0.190735652819;  AUC - 0.725019633486;\n",
      " 19 trees: MSE ~ 0.190559821654;  AUC - 0.725503392446;\n",
      " 20 trees: MSE ~ 0.19015178399;  AUC - 0.726782981808;\n",
      " 21 trees: MSE ~ 0.189969818071;  AUC - 0.727467109859;\n",
      " 22 trees: MSE ~ 0.189385312593;  AUC - 0.729277644449;\n",
      " 23 trees: MSE ~ 0.189020468919;  AUC - 0.730705235329;\n",
      " 24 trees: MSE ~ 0.188821682177;  AUC - 0.731438506596;\n",
      " 25 trees: MSE ~ 0.188486752464;  AUC - 0.732542678275;\n",
      " 26 trees: MSE ~ 0.188033370208;  AUC - 0.733893547843;\n",
      " 27 trees: MSE ~ 0.187571896225;  AUC - 0.735356835101;\n",
      " 28 trees: MSE ~ 0.187078343629;  AUC - 0.737266644161;\n",
      " 29 trees: MSE ~ 0.186658419382;  AUC - 0.7386122003;\n",
      " 30 trees: MSE ~ 0.186391525147;  AUC - 0.739422474476;\n",
      " 31 trees: MSE ~ 0.18622543043;  AUC - 0.739932781008;\n",
      " 32 trees: MSE ~ 0.18583633024;  AUC - 0.741403423833;\n",
      " 33 trees: MSE ~ 0.18492039391;  AUC - 0.744917079575;\n",
      " 34 trees: MSE ~ 0.184494077664;  AUC - 0.746251933674;\n",
      " 35 trees: MSE ~ 0.184304292264;  AUC - 0.746945978584;\n",
      " 36 trees: MSE ~ 0.184124708451;  AUC - 0.747521559244;\n",
      " 37 trees: MSE ~ 0.183714536098;  AUC - 0.748822861198;\n",
      " 38 trees: MSE ~ 0.183434944701;  AUC - 0.750093950809;\n",
      " 39 trees: MSE ~ 0.182994294974;  AUC - 0.751615894874;\n",
      " 40 trees: MSE ~ 0.182735258444;  AUC - 0.752572450286;\n",
      " 41 trees: MSE ~ 0.18261703035;  AUC - 0.753008623269;\n",
      " 42 trees: MSE ~ 0.182228625269;  AUC - 0.754084201456;\n",
      " 43 trees: MSE ~ 0.181886668526;  AUC - 0.755135412338;\n",
      " 44 trees: MSE ~ 0.181462743391;  AUC - 0.756408920217;\n",
      " 45 trees: MSE ~ 0.181055696302;  AUC - 0.757863055332;\n",
      " 46 trees: MSE ~ 0.18091839134;  AUC - 0.758408979725;\n",
      " 47 trees: MSE ~ 0.180771070002;  AUC - 0.75892972227;\n",
      " 48 trees: MSE ~ 0.180591449879;  AUC - 0.759358972536;\n",
      " 49 trees: MSE ~ 0.180532775985;  AUC - 0.75954743112;\n",
      " 50 trees: MSE ~ 0.180287607523;  AUC - 0.760385228895;\n",
      " 51 trees: MSE ~ 0.179925602659;  AUC - 0.761683531591;\n",
      " 52 trees: MSE ~ 0.179799598153;  AUC - 0.762146727036;\n",
      " 53 trees: MSE ~ 0.179411966014;  AUC - 0.763419239314;\n",
      " 54 trees: MSE ~ 0.179002396592;  AUC - 0.764831836794;\n",
      " 55 trees: MSE ~ 0.178886928355;  AUC - 0.765158685656;\n",
      " 56 trees: MSE ~ 0.178505903761;  AUC - 0.766516155862;\n",
      " 57 trees: MSE ~ 0.178269754959;  AUC - 0.767116610524;\n",
      " 58 trees: MSE ~ 0.178114351348;  AUC - 0.767551422229;\n",
      " 59 trees: MSE ~ 0.177932376357;  AUC - 0.768197906297;\n",
      " 60 trees: MSE ~ 0.177633384566;  AUC - 0.769526561025;\n",
      " 61 trees: MSE ~ 0.177389637277;  AUC - 0.770325493226;\n",
      " 62 trees: MSE ~ 0.177220443288;  AUC - 0.770864722448;\n",
      " 63 trees: MSE ~ 0.176937740869;  AUC - 0.771592778378;\n",
      " 64 trees: MSE ~ 0.176713350168;  AUC - 0.772404669404;\n",
      " 65 trees: MSE ~ 0.176407217717;  AUC - 0.773268427312;\n",
      " 66 trees: MSE ~ 0.176047309706;  AUC - 0.774508439544;\n",
      " 67 trees: MSE ~ 0.175870144523;  AUC - 0.775037504119;\n",
      " 68 trees: MSE ~ 0.175728626843;  AUC - 0.775339795052;\n",
      " 69 trees: MSE ~ 0.175534631685;  AUC - 0.775934858657;\n",
      " 70 trees: MSE ~ 0.175319022454;  AUC - 0.776539635374;\n",
      " 71 trees: MSE ~ 0.175259140234;  AUC - 0.77669282487;\n",
      " 72 trees: MSE ~ 0.174977605973;  AUC - 0.777431522783;\n",
      " 73 trees: MSE ~ 0.174638753704;  AUC - 0.778470665232;\n",
      " 74 trees: MSE ~ 0.174502343007;  AUC - 0.778876324699;\n",
      " 75 trees: MSE ~ 0.174148303845;  AUC - 0.779964331212;\n",
      " 76 trees: MSE ~ 0.173750684177;  AUC - 0.781040732393;\n",
      " 77 trees: MSE ~ 0.173521594139;  AUC - 0.781688384891;\n",
      " 78 trees: MSE ~ 0.173415239772;  AUC - 0.782087143217;\n",
      " 79 trees: MSE ~ 0.173233913541;  AUC - 0.782450950752;\n",
      " 80 trees: MSE ~ 0.173139848443;  AUC - 0.782720797693;\n",
      " 81 trees: MSE ~ 0.172887385594;  AUC - 0.783246672164;\n",
      " 82 trees: MSE ~ 0.172631969488;  AUC - 0.7838906114;\n",
      " 83 trees: MSE ~ 0.172393634131;  AUC - 0.784653827871;\n",
      " 84 trees: MSE ~ 0.172045365112;  AUC - 0.785571859141;\n",
      " 85 trees: MSE ~ 0.171654954413;  AUC - 0.786954582594;\n",
      " 86 trees: MSE ~ 0.171384395542;  AUC - 0.78789875083;\n",
      " 87 trees: MSE ~ 0.171222798096;  AUC - 0.788477369006;\n",
      " 88 trees: MSE ~ 0.170977151159;  AUC - 0.789185415719;\n",
      " 89 trees: MSE ~ 0.170732966872;  AUC - 0.789808730049;\n",
      " 90 trees: MSE ~ 0.170534834898;  AUC - 0.790474094275;\n",
      " 91 trees: MSE ~ 0.170378386506;  AUC - 0.790893997549;\n",
      " 92 trees: MSE ~ 0.170221333453;  AUC - 0.791455525692;\n",
      " 93 trees: MSE ~ 0.170085571151;  AUC - 0.79191549967;\n",
      " 94 trees: MSE ~ 0.169937942392;  AUC - 0.792432075417;\n",
      " 95 trees: MSE ~ 0.169635483545;  AUC - 0.793281006971;\n",
      " 96 trees: MSE ~ 0.169419569657;  AUC - 0.793883638565;\n",
      " 97 trees: MSE ~ 0.169195241623;  AUC - 0.794612337987;\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ea1009ad3954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[1;31m#print \" {} trees: MSE ~ {};  AUC - {};\".format(l+1, ((Yv-Pv2)**2).mean(), auc(Pv2,Yv) )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\" {} trees: MSE ~ {};  AUC - {};\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mPX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[1;31m#print \"AUC: \", auc(Pv2,Yv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ea1009ad3954>\u001b[0m in \u001b[0;36mauc\u001b[0;34m(soft, Y)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdif\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_soft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mr1\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdif\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mr2\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mr1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mr1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mrnk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdif\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Mar 10 13:56:58 2017\n",
    "\n",
    "@author: Brian\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import mltools as ml\n",
    "import matplotlib.pyplot as plt   # use matplotlib for plotting with inline plots\n",
    "\n",
    "X = np.genfromtxt(\"data/X_train.txt\",delimiter=' ')\n",
    "Y = np.genfromtxt(\"data/Y_train.txt\",delimiter=' ')\n",
    "Xt,Xv,Yt,Yv = ml.splitData(X,Y,0.80)\n",
    "\n",
    "Xe = np.genfromtxt('data/X_test.txt',delimiter=' ')\n",
    "\n",
    "\n",
    "def auc(soft,Y):\n",
    "    \"\"\"Manual AUC function for applying to soft prediction vectors\"\"\"\n",
    "    indices = np.argsort(soft)         # sort data by score value\n",
    "    Y = Y[indices]\n",
    "    sorted_soft = soft[indices]\n",
    "    \n",
    "    # compute rank (averaged for ties) of sorted data\n",
    "    dif = np.hstack( ([True],np.diff(sorted_soft)!=0,[True]) )\n",
    "    r1  = np.argwhere(dif).flatten()\n",
    "    r2  = r1[0:-1] + 0.5*(r1[1:]-r1[0:-1]) + 0.5\n",
    "    rnk = r2[np.cumsum(dif[:-1])-1]\n",
    "    \n",
    "    # number of true negatives and positives\n",
    "    n0,n1 = sum(Y == 0), sum(Y == 1)\n",
    "    \n",
    "    # compute AUC using Mann-Whitney U statistic\n",
    "    result = (np.sum(rnk[Y == 1]) - n1 * (n1 + 1.0) / 2.0) / n1 / n0\n",
    "    return result\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "nUse= 300\n",
    "# mu = Yt.mean()\n",
    "# dY = Yt - mu\n",
    "mu = Y.mean()\n",
    "dY = Y - mu\n",
    "\n",
    "step = 0.5\n",
    "PX = np.zeros((X.shape[0],))+mu\n",
    "Pt2 = np.zeros((Xt.shape[0],))+mu\n",
    "Pv2 = np.zeros((Xv.shape[0],))+mu\n",
    "Pe2 = np.zeros((Xe.shape[0],))+mu\n",
    "\n",
    "np.random.seed(0)\n",
    "for l in range(nUse):             # this is a lot faster than the bagging loop:\n",
    "    # Better: set dY = gradient of loss at soft predictions Pt\n",
    "    #tree = ml.dtree.treeRegress(Xt,dY[:,np.newaxis], maxDepth=5)  # train and save learner\n",
    "    tree = ml.dtree.treeRegress(X,dY[:,np.newaxis], maxDepth=5)  # train and save learner\n",
    "\n",
    "#     Pt2 += step*tree.predict(Xt)[:,0]       # predict on training data\n",
    "    PX += step*tree.predict(X)[:,0]       # predict on training data\n",
    "#     Pv2 += step*tree.predict(Xv)[:,0]        #    and validation data\n",
    "    Pe2 += step*tree.predict(Xe)[:,0]        #    and test data\n",
    "    dY  -= step*tree.predict(X)[:,0]        # update residual for next learner\n",
    "\n",
    "    #print \" {} trees: MSE ~ {};  AUC - {};\".format(l+1, ((Yv-Pv2)**2).mean(), auc(Pv2,Yv) )\n",
    "    print \" {} trees: MSE ~ {};  AUC - {};\".format(l+1, ((Y-PX)**2).mean(), auc(PX,Y) )\n",
    "\n",
    "#print \"AUC: \", auc(Pv2,Yv)\n",
    "#np.savetxt('kaggle prediction.txt',np.vstack( (np.arange(len(Pe2)) , Pe2) ).T,'%d, %.2f',header='ID,Prob1',comments='',delimiter=',');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "aucEnsemble() takes exactly 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2c53fa8ebf59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mkaggleEnsemble\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainEnsemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnBag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[1;32mprint\u001b[0m \u001b[0maucEnsemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0maucEnsemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: aucEnsemble() takes exactly 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "from numpy import atleast_2d as twod\n",
    "import mltools as ml\n",
    "import numpy as np\n",
    "\n",
    "def trainEnsemble(X, Y, nBag):\n",
    "    \"\"\" Train and return an ensemble of nBag learners \"\"\"\n",
    "    M,d = X.shape\n",
    "    ensemble = [None] * nBag\n",
    "    \n",
    "    for i in range(nBag):\n",
    "        Xi,Yi = ml.bootstrapData(X,Y,M)\n",
    "        ensemble[i] = ml.dtree.treeClassify(Xi, Yi, maxDepth=20, minLeaf=4, nFeatures=np.sqrt(d))\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def predictSoftEnsemble(X, E):\n",
    "    \"\"\"Make soft predictions on the data in X\n",
    "\n",
    "    Args:\n",
    "        X (arr): MxN numpy array containing M data points of N features each\n",
    "        E (arr): M array of M learners \n",
    "\n",
    "    Returns:\n",
    "        arr : M,C array of C class probabiities for each data point\n",
    "    \"\"\"\n",
    "    mTest = X.shape[0]\n",
    "    predict = np.zeros( (mTest, len(E)) ) \n",
    "    \n",
    "    for i in range(len(E)): # Apply each classifier \n",
    "        predict[:,i] = E[i].predictSoft(X)[:,1]  \n",
    "        \n",
    "    return np.mean(predict, axis=1)\n",
    "    \n",
    "    \n",
    "def aucEnsemble(X, Y, E):\n",
    "    \"\"\"Compute the area under the roc curve on the given test data.\n",
    "\n",
    "    Args: \n",
    "        X (arr): MxN numpy array containing M data points of N features each\n",
    "        Y (arr): M, or M,1 array of target class values for each data point\n",
    "        E (arr): M array of M learners \n",
    "\n",
    "    Returns:\n",
    "        float: Area under the ROC curve\n",
    "\n",
    "    This method only works on binary classifiers. \n",
    "    Modified from mltools. Class is either 0 or 1.\n",
    "    \"\"\"\n",
    "    soft = predictSoftEnsemble(X, E)\n",
    "    n,d = twod(soft).shape             # ensure soft is the correct shape\n",
    "    soft = soft.flatten() if n==1 else soft.T.flatten()\n",
    "\n",
    "    indices = np.argsort(soft)         # sort data by score value\n",
    "    Y = Y[indices]\n",
    "    sorted_soft = soft[indices]\n",
    "\n",
    "    # compute rank (averaged for ties) of sorted data\n",
    "    dif = np.hstack( ([True],np.diff(sorted_soft)!=0,[True]) )\n",
    "    r1  = np.argwhere(dif).flatten()\n",
    "    r2  = r1[0:-1] + 0.5*(r1[1:]-r1[0:-1]) + 0.5\n",
    "    rnk = r2[np.cumsum(dif[:-1])-1]\n",
    "\n",
    "    # number of true negatives and positives\n",
    "    n0,n1 = sum(Y == 0), sum(Y == 1)\n",
    "\n",
    "    if n0 == 0 or n1 == 0:\n",
    "      raise ValueError('Data of both class values not found')\n",
    "\n",
    "    # compute AUC using Mann-Whitney U statistic\n",
    "    result = (np.sum(rnk[Y == 1]) - n1 * (n1 + 1.0) / 2.0) / n1 / n0\n",
    "    return result\n",
    "\n",
    "X = np.genfromtxt(\"data/X_train.txt\",delimiter=' ')\n",
    "Y = np.genfromtxt(\"data/Y_train.txt\",delimiter=' ')\n",
    "#Xe = np.genfromtxt('data/X_test.txt',delimiter=' ')\n",
    "Xt,Xv,Yt,Yv = ml.splitData(X,Y,0.80)\n",
    "\n",
    "nBag = 26\n",
    "kaggleEnsemble = trainEnsemble(X, Y, nBag)\n",
    "\n",
    "print aucEnsemble(Xt, Yt)\n",
    "print aucEnsemble(Xv, Yv)\n",
    "\n",
    "Xte = np.genfromtxt(\"data/X_test.txt\", delimiter=None)\n",
    "ForestPred = predictSoftEnsemble(Xte, kaggleEnsemble)\n",
    "np.savetxt('Yhat_Forest.txt',\n",
    "    np.vstack( (np.arange(len(ForestPred)) , ForestPred) ).T,\n",
    "    '%d, %.2f',header='ID,Prob1',comments='',delimiter=',');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:\n",
      "[ 0.48949339  0.14610884  0.63234873 ...,  0.07059832  0.43257053\n",
      "  0.32292375]\n",
      "Forest:\n",
      "[ 0.33430875  0.28370882  0.43512753 ...,  0.63319355  0.54238522\n",
      "  0.17470337]\n",
      "Boost:\n",
      "[ 0.3   0.38  0.43 ...,  0.75  0.8   0.37]\n",
      "\n",
      "Average:\n",
      "[ 0.37460071  0.26993922  0.49915875 ...,  0.48459729  0.59165191\n",
      "  0.28920904]\n"
     ]
    }
   ],
   "source": [
    "BoostData = np.genfromtxt(\"data/Boosted Data.txt\",delimiter=',')\n",
    "\n",
    "BoostPred = BoostData[:,1]\n",
    "NNPred = YtePred\n",
    "\n",
    "print \"NN:\"\n",
    "print NNPred\n",
    "print \"Forest:\"\n",
    "print ForestPred\n",
    "print \"Boost:\"\n",
    "print BoostPred\n",
    "print\n",
    "print 'Average:'\n",
    "Average = (NNPred + ForestPred + BoostPred)/3\n",
    "print Average\n",
    "\n",
    "np.savetxt('Yhat_Avg.txt',\n",
    "    np.vstack( (np.arange(len(Average)) , Average) ).T,\n",
    "    '%d, %.2f',header='ID,Prob1',comments='',delimiter=',');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aucEnsemble' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aa58643c95c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mprint\u001b[0m \u001b[0maucEnsemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0maucEnsemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aucEnsemble' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923261855813\n",
      "0.925460912236\n"
     ]
    }
   ],
   "source": [
    "print aucEnsemble(Xt, Yt, kaggleEnsemble)\n",
    "print aucEnsemble(Xv, Yv, kaggleEnsemble)\n",
    "\n",
    "Xte = np.genfromtxt(\"data/X_test.txt\", delimiter=None)\n",
    "ForestPred = predictSoftEnsemble(Xte, kaggleEnsemble)\n",
    "np.savetxt('Yhat_Forest.txt',\n",
    "    np.vstack( (np.arange(len(ForestPred)) , ForestPred) ).T,\n",
    "    '%d, %.2f',header='ID,Prob1',comments='',delimiter=',');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92546091223562021"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#NNData = np.genfromtxt(\"data/Boosted Data.txt\",delimiter=',')\n",
    "\n",
    "\n",
    "Ytpred = predictSoftEnsemble(Xt, kaggleEnsemble)\n",
    "Yvpred = predictSoftEnsemble(Xv, kaggleEnsemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923261855813\n",
      "0.925460912236\n"
     ]
    }
   ],
   "source": [
    "print roc_auc_score(Yt, Ytpred)\n",
    "print roc_auc_score(Yv, Yvpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
