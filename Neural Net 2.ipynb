{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.loadtxt('data/X_train.txt')\n",
    "Y = np.loadtxt('data/Y_train.txt')\n",
    "Xe = np.loadtxt('data/X_test.txt')\n",
    "\n",
    "Xt, Xv = X[0:50000], X[50000:100000]\n",
    "Yt, Yv = Y[0:50000], Y[50000:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(Xt)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "Xt_new = scaler.transform(Xt)\n",
    "Xv_new = scaler.transform(Xv)\n",
    "# X_new = scaler.transform(X)\n",
    "# Xe_new = scaler.transform(Xe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62004090\n",
      "Iteration 2, loss = 0.60674451\n",
      "Iteration 3, loss = 0.60432652\n",
      "Iteration 4, loss = 0.60261500\n",
      "Iteration 5, loss = 0.60245245\n",
      "Iteration 6, loss = 0.60206960\n",
      "Iteration 7, loss = 0.60094842\n",
      "Iteration 8, loss = 0.60069294\n",
      "Iteration 9, loss = 0.60063040\n",
      "Iteration 10, loss = 0.59956670\n",
      "Iteration 11, loss = 0.59992462\n",
      "Iteration 12, loss = 0.59887041\n",
      "Iteration 13, loss = 0.59842297\n",
      "Iteration 14, loss = 0.59767239\n",
      "Iteration 15, loss = 0.59696439\n",
      "Iteration 16, loss = 0.59624247\n",
      "Iteration 17, loss = 0.59576941\n",
      "Iteration 18, loss = 0.59493012\n",
      "Iteration 19, loss = 0.59428373\n",
      "Iteration 20, loss = 0.59440670\n",
      "Iteration 21, loss = 0.59382164\n",
      "Iteration 22, loss = 0.59339910\n",
      "Iteration 23, loss = 0.59285527\n",
      "Iteration 24, loss = 0.59221750\n",
      "Iteration 25, loss = 0.59254800\n",
      "Iteration 26, loss = 0.59241129\n",
      "Iteration 27, loss = 0.59124606\n",
      "Iteration 28, loss = 0.59176354\n",
      "Iteration 29, loss = 0.59106062\n",
      "Iteration 30, loss = 0.59046958\n",
      "Iteration 31, loss = 0.59043176\n",
      "Iteration 32, loss = 0.59042433\n",
      "Iteration 33, loss = 0.59066971\n",
      "Iteration 34, loss = 0.58940960\n",
      "Iteration 35, loss = 0.58933307\n",
      "Iteration 36, loss = 0.58943020\n",
      "Iteration 37, loss = 0.58941201\n",
      "Iteration 38, loss = 0.58904591\n",
      "Iteration 39, loss = 0.58911636\n",
      "Iteration 40, loss = 0.58852784\n",
      "Iteration 41, loss = 0.58802421\n",
      "Iteration 42, loss = 0.58795787\n",
      "Iteration 43, loss = 0.58858309\n",
      "Iteration 44, loss = 0.58739704\n",
      "Iteration 45, loss = 0.58703708\n",
      "Iteration 46, loss = 0.58697152\n",
      "Iteration 47, loss = 0.58627400\n",
      "Iteration 48, loss = 0.58690331\n",
      "Iteration 49, loss = 0.58640016\n",
      "Iteration 50, loss = 0.58643896\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.002, max_iter=1000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=1e-10, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# You should monitor its performance, preferably on both training & validation data, during backpropagation, \n",
    "# and verify that the training process is working properly and converging to a reasonable performance value \n",
    "# (e.g., comparably to other methods).  Start with few layers (2-3) and moderate numbers of hidden nodes (100-1000) per layer; \n",
    "# within these settings you can work to make sure your model is training adequately.\n",
    "\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100,100,100), activation='tanh', solver='adam', max_iter=1000, shuffle=True, \n",
    "#                     learning_rate='invscaling', learning_rate_init=0.0001, power_t=0.5, tol=1e-10, verbose=True, \n",
    "#                     early_stopping=True)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), activation='logistic', solver='adam', max_iter=1000, shuffle=True, \n",
    "                    learning_rate_init=0.002, tol=1e-10, verbose=True, early_stopping=False)\n",
    "\n",
    "clf.fit(Xt_new,Yt)\n",
    "#clf.fit(X_new,Y)\n",
    "\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# X_train, X_test, y_train, y_test = train_test_split(Xt_new, Yt, test_size=0.1, random_state=42)\n",
    "# clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')\n",
    "# clf_isotonic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score = 0.87244\n",
      "Training AUC = 0.940548555962\n",
      "\n",
      "Validation Score = 0.67834\n",
      "Validation AUC = 0.682307093292\n"
     ]
    }
   ],
   "source": [
    "print 'Training Score =',clf.score(Xt_new,Yt)\n",
    "Ytpred = clf.predict_proba(Xt_new)\n",
    "print 'Training AUC =',roc_auc_score(Yt, Ytpred[:,1])\n",
    "print\n",
    "print 'Validation Score =',clf.score(Xv_new,Yv)\n",
    "Yvpred = clf.predict_proba(Xv_new)\n",
    "print 'Validation AUC =',roc_auc_score(Yv, Yvpred[:,1])\n",
    "\n",
    "# print 'Training Score =',clf_isotonic.score(Xt_new,Yt)\n",
    "# Ytpred = clf_isotonic.predict_proba(Xt_new)\n",
    "# print 'Training AUC =',roc_auc_score(Yt, Ytpred[:,1])\n",
    "# print\n",
    "# print 'Validation Score =',clf_isotonic.score(Xv_new,Yv)\n",
    "# Yvpred = clf_isotonic.predict_proba(Xv_new)\n",
    "# print 'Validation AUC =',roc_auc_score(Yv, Yvpred[:,1])\n",
    "# clf_isotonic.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YtePred = clf.predict_proba(Xe_new)[:,1]\n",
    "# np.savetxt('Yhat_NNet.txt',\n",
    "#     np.vstack( (np.arange(len(YtePred)) , YtePred) ).T,\n",
    "#     '%d, %.2f',header='ID,Prob1',comments='',delimiter=',');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
