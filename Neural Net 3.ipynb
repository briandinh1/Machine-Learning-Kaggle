{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mltools as ml;\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.loadtxt('data/X_train.txt')\n",
    "Y = np.loadtxt('data/Y_train.txt')\n",
    "Xe = np.loadtxt('data/X_test.txt')\n",
    "\n",
    "Xt,Xv,Yt,Yv = ml.splitData(X,Y,0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "Xt_new = scaler.transform(Xt)\n",
    "Xv_new = scaler.transform(Xv)\n",
    "X_new = scaler.transform(X)\n",
    "Xe_new = scaler.transform(Xe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60610069\n",
      "Iteration 2, loss = 0.60087486\n",
      "Iteration 3, loss = 0.59781261\n",
      "Iteration 4, loss = 0.59616382\n",
      "Iteration 5, loss = 0.59463296\n",
      "Iteration 6, loss = 0.59377377\n",
      "Iteration 7, loss = 0.59258137\n",
      "Iteration 8, loss = 0.59181808\n",
      "Iteration 9, loss = 0.59129139\n",
      "Iteration 10, loss = 0.59040125\n",
      "Iteration 11, loss = 0.58978247\n",
      "Iteration 12, loss = 0.58920697\n",
      "Iteration 13, loss = 0.58886809\n",
      "Iteration 14, loss = 0.58820649\n",
      "Iteration 15, loss = 0.58761779\n",
      "Iteration 16, loss = 0.58748096\n",
      "Iteration 17, loss = 0.58665797\n",
      "Iteration 18, loss = 0.58651142\n",
      "Iteration 19, loss = 0.58589200\n",
      "Iteration 20, loss = 0.58585700\n",
      "Iteration 21, loss = 0.58549475\n",
      "Iteration 22, loss = 0.58503040\n",
      "Iteration 23, loss = 0.58470088\n",
      "Iteration 24, loss = 0.58422081\n",
      "Iteration 25, loss = 0.58379397\n",
      "Iteration 26, loss = 0.58342078\n",
      "Iteration 27, loss = 0.58294713\n",
      "Iteration 28, loss = 0.58274714\n",
      "Iteration 29, loss = 0.58261513\n",
      "Iteration 30, loss = 0.58210248\n",
      "Iteration 31, loss = 0.58158301\n",
      "Iteration 32, loss = 0.58141096\n",
      "Iteration 33, loss = 0.58076412\n",
      "Iteration 34, loss = 0.58056481\n",
      "Iteration 35, loss = 0.58034094\n",
      "Iteration 36, loss = 0.57975685\n",
      "Iteration 37, loss = 0.57978426\n",
      "Iteration 38, loss = 0.57927291\n",
      "Iteration 39, loss = 0.57898553\n",
      "Iteration 40, loss = 0.57899422\n",
      "Iteration 41, loss = 0.57868915\n",
      "Iteration 42, loss = 0.57815057\n",
      "Iteration 43, loss = 0.57784331\n",
      "Iteration 44, loss = 0.57735372\n",
      "Iteration 45, loss = 0.57730555\n",
      "Iteration 46, loss = 0.57688709\n",
      "Iteration 47, loss = 0.57639782\n",
      "Iteration 48, loss = 0.57642583\n",
      "Iteration 49, loss = 0.57608584\n",
      "Iteration 50, loss = 0.57560529\n",
      "Iteration 51, loss = 0.57553846\n",
      "Iteration 52, loss = 0.57506905\n",
      "Iteration 53, loss = 0.57497764\n",
      "Iteration 54, loss = 0.57472517\n",
      "Iteration 55, loss = 0.57413153\n",
      "Iteration 56, loss = 0.57403727\n",
      "Iteration 57, loss = 0.57351132\n",
      "Iteration 58, loss = 0.57356406\n",
      "Iteration 59, loss = 0.57318611\n",
      "Iteration 60, loss = 0.57298815\n",
      "Iteration 61, loss = 0.57277547\n",
      "Iteration 62, loss = 0.57249792\n",
      "Iteration 63, loss = 0.57223198\n",
      "Iteration 64, loss = 0.57175797\n",
      "Iteration 65, loss = 0.57173052\n",
      "Iteration 66, loss = 0.57141971\n",
      "Iteration 67, loss = 0.57077376\n",
      "Iteration 68, loss = 0.57068642\n",
      "Iteration 69, loss = 0.57060797\n",
      "Iteration 70, loss = 0.57056048\n",
      "Iteration 71, loss = 0.57006332\n",
      "Iteration 72, loss = 0.56984240\n",
      "Iteration 73, loss = 0.56923801\n",
      "Iteration 74, loss = 0.56928333\n",
      "Iteration 75, loss = 0.56899617\n",
      "Iteration 76, loss = 0.56883282\n",
      "Iteration 77, loss = 0.56854291\n",
      "Iteration 78, loss = 0.56817252\n",
      "Iteration 79, loss = 0.56790137\n",
      "Iteration 80, loss = 0.56767023\n",
      "Iteration 81, loss = 0.56747479\n",
      "Iteration 82, loss = 0.56701676\n",
      "Iteration 83, loss = 0.56670740\n",
      "Iteration 84, loss = 0.56665815\n",
      "Iteration 85, loss = 0.56619992\n",
      "Iteration 86, loss = 0.56583451\n",
      "Iteration 87, loss = 0.56589898\n",
      "Iteration 88, loss = 0.56515262\n",
      "Iteration 89, loss = 0.56502506\n",
      "Iteration 90, loss = 0.56518114\n",
      "Iteration 91, loss = 0.56451388\n",
      "Iteration 92, loss = 0.56452762\n",
      "Iteration 93, loss = 0.56413616\n",
      "Iteration 94, loss = 0.56362326\n",
      "Iteration 95, loss = 0.56348694\n",
      "Iteration 96, loss = 0.56298064\n",
      "Iteration 97, loss = 0.56286495\n",
      "Iteration 98, loss = 0.56266757\n",
      "Iteration 99, loss = 0.56236933\n",
      "Iteration 100, loss = 0.56189970\n",
      "Iteration 101, loss = 0.56173516\n",
      "Iteration 102, loss = 0.56155925\n",
      "Iteration 103, loss = 0.56134492\n",
      "Iteration 104, loss = 0.56089166\n",
      "Iteration 105, loss = 0.56064483\n",
      "Iteration 106, loss = 0.56023815\n",
      "Iteration 107, loss = 0.56010442\n",
      "Iteration 108, loss = 0.55958686\n",
      "Iteration 109, loss = 0.55927999\n",
      "Iteration 110, loss = 0.55911232\n",
      "Iteration 111, loss = 0.55895727\n",
      "Iteration 112, loss = 0.55845621\n",
      "Iteration 113, loss = 0.55824471\n",
      "Iteration 114, loss = 0.55792013\n",
      "Iteration 115, loss = 0.55759037\n",
      "Iteration 116, loss = 0.55737619\n",
      "Iteration 117, loss = 0.55695434\n",
      "Iteration 118, loss = 0.55690113\n",
      "Iteration 119, loss = 0.55635298\n",
      "Iteration 120, loss = 0.55620743\n",
      "Iteration 121, loss = 0.55601944\n",
      "Iteration 122, loss = 0.55547684\n",
      "Iteration 123, loss = 0.55526110\n",
      "Iteration 124, loss = 0.55506578\n",
      "Iteration 125, loss = 0.55487325\n",
      "Iteration 126, loss = 0.55460723\n",
      "Iteration 127, loss = 0.55412889\n",
      "Iteration 128, loss = 0.55387433\n",
      "Iteration 129, loss = 0.55340636\n",
      "Iteration 130, loss = 0.55338058\n",
      "Iteration 131, loss = 0.55296476\n",
      "Iteration 132, loss = 0.55242794\n",
      "Iteration 133, loss = 0.55242279\n",
      "Iteration 134, loss = 0.55203795\n",
      "Iteration 135, loss = 0.55182678\n",
      "Iteration 136, loss = 0.55154117\n",
      "Iteration 137, loss = 0.55118068\n",
      "Iteration 138, loss = 0.55074922\n",
      "Iteration 139, loss = 0.55075621\n",
      "Iteration 140, loss = 0.55046293\n",
      "Iteration 141, loss = 0.54998433\n",
      "Iteration 142, loss = 0.54970510\n",
      "Iteration 143, loss = 0.54929926\n",
      "Iteration 144, loss = 0.54910092\n",
      "Iteration 145, loss = 0.54885114\n",
      "Iteration 146, loss = 0.54835306\n",
      "Iteration 147, loss = 0.54809648\n",
      "Iteration 148, loss = 0.54778645\n",
      "Iteration 149, loss = 0.54747254\n",
      "Iteration 150, loss = 0.54728708\n",
      "Iteration 151, loss = 0.54693760\n",
      "Iteration 152, loss = 0.54650773\n",
      "Iteration 153, loss = 0.54636677\n",
      "Iteration 154, loss = 0.54604546\n",
      "Iteration 155, loss = 0.54563443\n",
      "Iteration 156, loss = 0.54549553\n",
      "Iteration 157, loss = 0.54526489\n",
      "Iteration 158, loss = 0.54499203\n",
      "Iteration 159, loss = 0.54452710\n",
      "Iteration 160, loss = 0.54409108\n",
      "Iteration 161, loss = 0.54394721\n",
      "Iteration 162, loss = 0.54347939\n",
      "Iteration 163, loss = 0.54337949\n",
      "Iteration 164, loss = 0.54300763\n",
      "Iteration 165, loss = 0.54276948\n",
      "Iteration 166, loss = 0.54233631\n",
      "Iteration 167, loss = 0.54205354\n",
      "Iteration 168, loss = 0.54166135\n",
      "Iteration 169, loss = 0.54142441\n",
      "Iteration 170, loss = 0.54095923\n",
      "Iteration 171, loss = 0.54058367\n",
      "Iteration 172, loss = 0.54046284\n",
      "Iteration 173, loss = 0.54015400\n",
      "Iteration 174, loss = 0.53986372\n",
      "Iteration 175, loss = 0.53920621\n",
      "Iteration 176, loss = 0.53911767\n",
      "Iteration 177, loss = 0.53870318\n",
      "Iteration 178, loss = 0.53868023\n",
      "Iteration 179, loss = 0.53808564\n",
      "Iteration 180, loss = 0.53789438\n",
      "Iteration 181, loss = 0.53749571\n",
      "Iteration 182, loss = 0.53696584\n",
      "Iteration 183, loss = 0.53700368\n",
      "Iteration 184, loss = 0.53638054\n",
      "Iteration 185, loss = 0.53613783\n",
      "Iteration 186, loss = 0.53586023\n",
      "Iteration 187, loss = 0.53560623\n",
      "Iteration 188, loss = 0.53526752\n",
      "Iteration 189, loss = 0.53481405\n",
      "Iteration 190, loss = 0.53456898\n",
      "Iteration 191, loss = 0.53431583\n",
      "Iteration 192, loss = 0.53390860\n",
      "Iteration 193, loss = 0.53352530\n",
      "Iteration 194, loss = 0.53321188\n",
      "Iteration 195, loss = 0.53294572\n",
      "Iteration 196, loss = 0.53252446\n",
      "Iteration 197, loss = 0.53256634\n",
      "Iteration 198, loss = 0.53166234\n",
      "Iteration 199, loss = 0.53175584\n",
      "Iteration 200, loss = 0.53125462\n",
      "Iteration 201, loss = 0.53077527\n",
      "Iteration 202, loss = 0.53069040\n",
      "Iteration 203, loss = 0.53013456\n",
      "Iteration 204, loss = 0.53002350\n",
      "Iteration 205, loss = 0.52984646\n",
      "Iteration 206, loss = 0.52947695\n",
      "Iteration 207, loss = 0.52912876\n",
      "Iteration 208, loss = 0.52882658\n",
      "Iteration 209, loss = 0.52841957\n",
      "Iteration 210, loss = 0.52800989\n",
      "Iteration 211, loss = 0.52761929\n",
      "Iteration 212, loss = 0.52741352\n",
      "Iteration 213, loss = 0.52710631\n",
      "Iteration 214, loss = 0.52664332\n",
      "Iteration 215, loss = 0.52631034\n",
      "Iteration 216, loss = 0.52588209\n",
      "Iteration 217, loss = 0.52558133\n",
      "Iteration 218, loss = 0.52531923\n",
      "Iteration 219, loss = 0.52500866\n",
      "Iteration 220, loss = 0.52470232\n",
      "Iteration 221, loss = 0.52443616\n",
      "Iteration 222, loss = 0.52397315\n",
      "Iteration 223, loss = 0.52347120\n",
      "Iteration 224, loss = 0.52340453\n",
      "Iteration 225, loss = 0.52289172\n",
      "Iteration 226, loss = 0.52264626\n",
      "Iteration 227, loss = 0.52231541\n",
      "Iteration 228, loss = 0.52202948\n",
      "Iteration 229, loss = 0.52177268\n",
      "Iteration 230, loss = 0.52123295\n",
      "Iteration 231, loss = 0.52113135\n",
      "Iteration 232, loss = 0.52067194\n",
      "Iteration 233, loss = 0.52035164\n",
      "Iteration 234, loss = 0.52006863\n",
      "Iteration 235, loss = 0.51959071\n",
      "Iteration 236, loss = 0.51930766\n",
      "Iteration 237, loss = 0.51904451\n",
      "Iteration 238, loss = 0.51834008\n",
      "Iteration 239, loss = 0.51832933\n",
      "Iteration 240, loss = 0.51773143\n",
      "Iteration 241, loss = 0.51741323\n",
      "Iteration 242, loss = 0.51705931\n",
      "Iteration 243, loss = 0.51695841\n",
      "Iteration 244, loss = 0.51648715\n",
      "Iteration 245, loss = 0.51626301\n",
      "Iteration 246, loss = 0.51599061\n",
      "Iteration 247, loss = 0.51543054\n",
      "Iteration 248, loss = 0.51499243\n",
      "Iteration 249, loss = 0.51466855\n",
      "Iteration 250, loss = 0.51430091\n",
      "Iteration 251, loss = 0.51403797\n",
      "Iteration 252, loss = 0.51389843\n",
      "Iteration 253, loss = 0.51361841\n",
      "Iteration 254, loss = 0.51296995\n",
      "Iteration 255, loss = 0.51270186\n",
      "Iteration 256, loss = 0.51233880\n",
      "Iteration 257, loss = 0.51185081\n",
      "Iteration 258, loss = 0.51167212\n",
      "Iteration 259, loss = 0.51099087\n",
      "Iteration 260, loss = 0.51106872\n",
      "Iteration 261, loss = 0.51071205\n",
      "Iteration 262, loss = 0.51013901\n",
      "Iteration 263, loss = 0.51000427\n",
      "Iteration 264, loss = 0.50972839\n",
      "Iteration 265, loss = 0.50906427\n",
      "Iteration 266, loss = 0.50862660\n",
      "Iteration 267, loss = 0.50879297\n",
      "Iteration 268, loss = 0.50805744\n",
      "Iteration 269, loss = 0.50775507\n",
      "Iteration 270, loss = 0.50712843\n",
      "Iteration 271, loss = 0.50720804\n",
      "Iteration 272, loss = 0.50643080\n",
      "Iteration 273, loss = 0.50630233\n",
      "Iteration 274, loss = 0.50606273\n",
      "Iteration 275, loss = 0.50536698\n",
      "Iteration 276, loss = 0.50521505\n",
      "Iteration 277, loss = 0.50496448\n",
      "Iteration 278, loss = 0.50428943\n",
      "Iteration 279, loss = 0.50424145\n",
      "Iteration 280, loss = 0.50372989\n",
      "Iteration 281, loss = 0.50340565\n",
      "Iteration 282, loss = 0.50298217\n",
      "Iteration 283, loss = 0.50280900\n",
      "Iteration 284, loss = 0.50239155\n",
      "Iteration 285, loss = 0.50196740\n",
      "Iteration 286, loss = 0.50168649\n",
      "Iteration 287, loss = 0.50099536\n",
      "Iteration 288, loss = 0.50081813\n",
      "Iteration 289, loss = 0.50051686\n",
      "Iteration 290, loss = 0.49994026\n",
      "Iteration 291, loss = 0.49985726\n",
      "Iteration 292, loss = 0.49940410\n",
      "Iteration 293, loss = 0.49915496\n",
      "Iteration 294, loss = 0.49887877\n",
      "Iteration 295, loss = 0.49828606\n",
      "Iteration 296, loss = 0.49788692\n",
      "Iteration 297, loss = 0.49764027\n",
      "Iteration 298, loss = 0.49730495\n",
      "Iteration 299, loss = 0.49689132\n",
      "Iteration 300, loss = 0.49640621\n",
      "Iteration 301, loss = 0.49616303\n",
      "Iteration 302, loss = 0.49588897\n",
      "Iteration 303, loss = 0.49544437\n",
      "Iteration 304, loss = 0.49496837\n",
      "Iteration 305, loss = 0.49462267\n",
      "Iteration 306, loss = 0.49431017\n",
      "Iteration 307, loss = 0.49408990\n",
      "Iteration 308, loss = 0.49344323\n",
      "Iteration 309, loss = 0.49319988\n",
      "Iteration 310, loss = 0.49283085\n",
      "Iteration 311, loss = 0.49272706\n",
      "Iteration 312, loss = 0.49219935\n",
      "Iteration 313, loss = 0.49191629\n",
      "Iteration 314, loss = 0.49107190\n",
      "Iteration 315, loss = 0.49088090\n",
      "Iteration 316, loss = 0.49066432\n",
      "Iteration 317, loss = 0.49023842\n",
      "Iteration 318, loss = 0.48974285\n",
      "Iteration 319, loss = 0.48959283\n",
      "Iteration 320, loss = 0.48933490\n",
      "Iteration 321, loss = 0.48899556\n",
      "Iteration 322, loss = 0.48839460\n",
      "Iteration 323, loss = 0.48831988\n",
      "Iteration 324, loss = 0.48774702\n",
      "Iteration 325, loss = 0.48729597\n",
      "Iteration 326, loss = 0.48709867\n",
      "Iteration 327, loss = 0.48651102\n",
      "Iteration 328, loss = 0.48641081\n",
      "Iteration 329, loss = 0.48598740\n",
      "Iteration 330, loss = 0.48547485\n",
      "Iteration 331, loss = 0.48524213\n",
      "Iteration 332, loss = 0.48473815\n",
      "Iteration 333, loss = 0.48433584\n",
      "Iteration 334, loss = 0.48386809\n",
      "Iteration 335, loss = 0.48377386\n",
      "Iteration 336, loss = 0.48330210\n",
      "Iteration 337, loss = 0.48290587\n",
      "Iteration 338, loss = 0.48262039\n",
      "Iteration 339, loss = 0.48219782\n",
      "Iteration 340, loss = 0.48204443\n",
      "Iteration 341, loss = 0.48136602\n",
      "Iteration 342, loss = 0.48132582\n",
      "Iteration 343, loss = 0.48064696\n",
      "Iteration 344, loss = 0.48073769\n",
      "Iteration 345, loss = 0.48011023\n",
      "Iteration 346, loss = 0.47959302\n",
      "Iteration 347, loss = 0.47907225\n",
      "Iteration 348, loss = 0.47902844\n",
      "Iteration 349, loss = 0.47852605\n",
      "Iteration 350, loss = 0.47821267\n",
      "Iteration 351, loss = 0.47785496\n",
      "Iteration 352, loss = 0.47778375\n",
      "Iteration 353, loss = 0.47699339\n",
      "Iteration 354, loss = 0.47665567\n",
      "Iteration 355, loss = 0.47622708\n",
      "Iteration 356, loss = 0.47638196\n",
      "Iteration 357, loss = 0.47566969\n",
      "Iteration 358, loss = 0.47517817\n",
      "Iteration 359, loss = 0.47507303\n",
      "Iteration 360, loss = 0.47456145\n",
      "Iteration 361, loss = 0.47445697\n",
      "Iteration 362, loss = 0.47387644\n",
      "Iteration 363, loss = 0.47342054\n",
      "Iteration 364, loss = 0.47312994\n",
      "Iteration 365, loss = 0.47279629\n",
      "Iteration 366, loss = 0.47259297\n",
      "Iteration 367, loss = 0.47209675\n",
      "Iteration 368, loss = 0.47172974\n",
      "Iteration 369, loss = 0.47150496\n",
      "Iteration 370, loss = 0.47084681\n",
      "Iteration 371, loss = 0.47044737\n",
      "Iteration 372, loss = 0.47029106\n",
      "Iteration 373, loss = 0.46988314\n",
      "Iteration 374, loss = 0.46960293\n",
      "Iteration 375, loss = 0.46933433\n",
      "Iteration 376, loss = 0.46895844\n",
      "Iteration 377, loss = 0.46860471\n",
      "Iteration 378, loss = 0.46826348\n",
      "Iteration 379, loss = 0.46791335\n",
      "Iteration 380, loss = 0.46722807\n",
      "Iteration 381, loss = 0.46689451\n",
      "Iteration 382, loss = 0.46666424\n",
      "Iteration 383, loss = 0.46646074\n",
      "Iteration 384, loss = 0.46593332\n",
      "Iteration 385, loss = 0.46546293\n",
      "Iteration 386, loss = 0.46544858\n",
      "Iteration 387, loss = 0.46488616\n",
      "Iteration 388, loss = 0.46446613\n",
      "Iteration 389, loss = 0.46408707\n",
      "Iteration 390, loss = 0.46386028\n",
      "Iteration 391, loss = 0.46376018\n",
      "Iteration 392, loss = 0.46317570\n",
      "Iteration 393, loss = 0.46273055\n",
      "Iteration 394, loss = 0.46233074\n",
      "Iteration 395, loss = 0.46189920\n",
      "Iteration 396, loss = 0.46188472\n",
      "Iteration 397, loss = 0.46140356\n",
      "Iteration 398, loss = 0.46108157\n",
      "Iteration 399, loss = 0.46061654\n",
      "Iteration 400, loss = 0.46033129\n",
      "Iteration 401, loss = 0.45997960\n",
      "Iteration 402, loss = 0.45983622\n",
      "Iteration 403, loss = 0.45940843\n",
      "Iteration 404, loss = 0.45887821\n",
      "Iteration 405, loss = 0.45853424\n",
      "Iteration 406, loss = 0.45793764\n",
      "Iteration 407, loss = 0.45780678\n",
      "Iteration 408, loss = 0.45737212\n",
      "Iteration 409, loss = 0.45732455\n",
      "Iteration 410, loss = 0.45678508\n",
      "Iteration 411, loss = 0.45670883\n",
      "Iteration 412, loss = 0.45598430\n",
      "Iteration 413, loss = 0.45557943\n",
      "Iteration 414, loss = 0.45522927\n",
      "Iteration 415, loss = 0.45505919\n",
      "Iteration 416, loss = 0.45465243\n",
      "Iteration 417, loss = 0.45436510\n",
      "Iteration 418, loss = 0.45386699\n",
      "Iteration 419, loss = 0.45360164\n",
      "Iteration 420, loss = 0.45327494\n",
      "Iteration 421, loss = 0.45286150\n",
      "Iteration 422, loss = 0.45260111\n",
      "Iteration 423, loss = 0.45219943\n",
      "Iteration 424, loss = 0.45213493\n",
      "Iteration 425, loss = 0.45148206\n",
      "Iteration 426, loss = 0.45119801\n",
      "Iteration 427, loss = 0.45087387\n",
      "Iteration 428, loss = 0.45050865\n",
      "Iteration 429, loss = 0.45018082\n",
      "Iteration 430, loss = 0.44983031\n",
      "Iteration 431, loss = 0.44928444\n",
      "Iteration 432, loss = 0.44922629\n",
      "Iteration 433, loss = 0.44884993\n",
      "Iteration 434, loss = 0.44841303\n",
      "Iteration 435, loss = 0.44833635\n",
      "Iteration 436, loss = 0.44774304\n",
      "Iteration 437, loss = 0.44739666\n",
      "Iteration 438, loss = 0.44711280\n",
      "Iteration 439, loss = 0.44666180\n",
      "Iteration 440, loss = 0.44635642\n",
      "Iteration 441, loss = 0.44621132\n",
      "Iteration 442, loss = 0.44560887\n",
      "Iteration 443, loss = 0.44546878\n",
      "Iteration 444, loss = 0.44478443\n",
      "Iteration 445, loss = 0.44450851\n",
      "Iteration 446, loss = 0.44438193\n",
      "Iteration 447, loss = 0.44383382\n",
      "Iteration 448, loss = 0.44353089\n",
      "Iteration 449, loss = 0.44308342\n",
      "Iteration 450, loss = 0.44293271\n",
      "Iteration 451, loss = 0.44261017\n",
      "Iteration 452, loss = 0.44228672\n",
      "Iteration 453, loss = 0.44183314\n",
      "Iteration 454, loss = 0.44155905\n",
      "Iteration 455, loss = 0.44121432\n",
      "Iteration 456, loss = 0.44088721\n",
      "Iteration 457, loss = 0.44055330\n",
      "Iteration 458, loss = 0.44042070\n",
      "Iteration 459, loss = 0.43991044\n",
      "Iteration 460, loss = 0.43958397\n",
      "Iteration 461, loss = 0.43914566\n",
      "Iteration 462, loss = 0.43892507\n",
      "Iteration 463, loss = 0.43846174\n",
      "Iteration 464, loss = 0.43812950\n",
      "Iteration 465, loss = 0.43794792\n",
      "Iteration 466, loss = 0.43736715\n",
      "Iteration 467, loss = 0.43721236\n",
      "Iteration 468, loss = 0.43675640\n",
      "Iteration 469, loss = 0.43644738\n",
      "Iteration 470, loss = 0.43605769\n",
      "Iteration 471, loss = 0.43592869\n",
      "Iteration 472, loss = 0.43556450\n",
      "Iteration 473, loss = 0.43518630\n",
      "Iteration 474, loss = 0.43474561\n",
      "Iteration 475, loss = 0.43448961\n",
      "Iteration 476, loss = 0.43399117\n",
      "Iteration 477, loss = 0.43375354\n",
      "Iteration 478, loss = 0.43345073\n",
      "Iteration 479, loss = 0.43308684\n",
      "Iteration 480, loss = 0.43289783\n",
      "Iteration 481, loss = 0.43257755\n",
      "Iteration 482, loss = 0.43221383\n",
      "Iteration 483, loss = 0.43219286\n",
      "Iteration 484, loss = 0.43148806\n",
      "Iteration 485, loss = 0.43096089\n",
      "Iteration 486, loss = 0.43084040\n",
      "Iteration 487, loss = 0.43034144\n",
      "Iteration 488, loss = 0.43002039\n",
      "Iteration 489, loss = 0.42979526\n",
      "Iteration 490, loss = 0.42938480\n",
      "Iteration 491, loss = 0.42891771\n",
      "Iteration 492, loss = 0.42861930\n",
      "Iteration 493, loss = 0.42859982\n",
      "Iteration 494, loss = 0.42812757\n",
      "Iteration 495, loss = 0.42784321\n",
      "Iteration 496, loss = 0.42759005\n",
      "Iteration 497, loss = 0.42732911\n",
      "Iteration 498, loss = 0.42668379\n",
      "Iteration 499, loss = 0.42640503\n",
      "Iteration 500, loss = 0.42628048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brian\\Anaconda2\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(200, 200, 200), learning_rate='constant',\n",
       "       learning_rate_init=0.0001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=1e-06, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# You should monitor its performance, preferably on both training & validation data, during backpropagation, \n",
    "# and verify that the training process is working properly and converging to a reasonable performance value \n",
    "# (e.g., comparably to other methods).  Start with few layers (2-3) and moderate numbers of hidden nodes (100-1000) per layer; \n",
    "# within these settings you can work to make sure your model is training adequately.\n",
    "\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100,100,100), activation='logistic', solver='adam', max_iter=5000, shuffle=True, \n",
    "#                     learning_rate='invscaling', learning_rate_init=0.0001, power_t=0.5, tol=1e-6, verbose=True, \n",
    "#                     early_stopping=True)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(200,200,200), activation='tanh', solver='adam', max_iter=500, shuffle=True, \n",
    "                    learning_rate_init=0.0001, tol=1e-6, verbose=True, early_stopping=False)\n",
    "\n",
    "clf.fit(X_new,Y)\n",
    "#clf.fit(Xt_new,Yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC = 0.870243603202\n",
      "Validation AUC = 0.869356507741\n"
     ]
    }
   ],
   "source": [
    "# print 'Training Score =',clf.score(Xt_new,Yt)\n",
    "Ytpred = clf.predict_proba(Xt_new)\n",
    "print 'Training AUC =',roc_auc_score(Yt, Ytpred[:,1])\n",
    "# print\n",
    "# print 'Validation Score =',clf.score(Xv_new,Yv)\n",
    "Yvpred = clf.predict_proba(Xv_new)\n",
    "print 'Validation AUC =',roc_auc_score(Yv, Yvpred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "YtePred = clf.predict_proba(Xe_new)[:,1]\n",
    "np.savetxt('Yhat_NNet3.txt',\n",
    "    np.vstack( (np.arange(len(YtePred)) , YtePred) ).T,\n",
    "    '%d, %.2f',header='ID,Prob1',comments='',delimiter=',');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
